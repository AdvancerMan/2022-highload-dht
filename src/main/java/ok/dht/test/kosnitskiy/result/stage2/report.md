ENVIRONMENT-DESCRIPTION
Тестирование производилось на 6-ядерном, 6-поточном процессоре. По этой причине было решено использовать 4 потока под
при тестировании wrk (и 64 connections, как сказано в задании), чтобы еще два ядра были не задействованы и использовались
под нужны системы, профайлера и так далее

GET-DESCRIPTION
Для тестов брался наихудший случай - настоящие ключи, перемешанные с несуществующими. В таких условиях базе зачастую
приходится проходиться сразу по всем файлам, проверяя наличие ключа в каждом из них, а так же не получается ничего
закешировать

PUT-DESCRIPTION
Для PUT мы генерировали разные ключи, чтобы гарантировать, что размер DAO будет увеличиваться и не будет 
перезаписываться одно и тоже значение кучу раз.

WRK-GET
Последовательные тесты показали значительное повышение производительности по сравнению с однопоточной реализацией,
используемой до этого. Раньше схожий по наполнению GET запрос показывал задержки при 600 запросах в секунду схожие
с задержками при 3000-4000 запросах в секунду сейчас.
Итого мы получили улучшение в 5-6 раз по сравнению с прошлой реализацией, при этом количество потоков, которое мы используем
увеличилось всего в 4 раза. Получается, мы не только получили прирост на производительность в многопотоке сугубо из-за
числа потоков, но и увеличили КПД этих потоков в целом! Это просто уберсладко!
Однако хочется отметить, что максимальное время исполнения все так же много выше среднего, это может быть результатом
того, что PUT запросы аллоцируют очень много памяти и поэтому шалости Garbage Collector так фатально нам ухудшают
статистику. Это доказывается еще и тем, что я ставил размер очереди очень маленьким (1 единица, фактически без очереди),
таким образом очередь не образовывалась, а все новые запросы просто отклонялись, и даже это не помогало нам приблизить 
время выполнения МAX запросов к среднему времени, так что предположение, что MAX время вызвано большим размером очереди
и толкучкой на выполнение, что там образуется, не подтвердилось.
Если вернуться к GC, то у меня есть теория, что нам могло бы с этой проблемой помочь создание системы запасной оперативной
памяти: когда garbage collector планирует чистить память - он сообщает об этой нашей программе, она начинает все аллоцировать
в запасной части памяти, а Garbage collector в это время в не используемой программой потоке, аккуратненько чистит эту
основную память потихоньку, потом они меняются местами и цикл повторяется.
Однако тут возникает новый вопрос - не лучше ли будет всю эту память и лишний поток просто заиспользовать для улучшения
средний производительности - ну тут все зависит от того что нам важнее: средняя скорость чуть повыше, или MAX значение
сильно поближе к среднему

WRK-PUT
С PUT мы можем заметить, что все тоже сладко, но не так сладко, как с GET.
Наш потолок нагрузки поднялся с 20к до 70к запросов в секунду. С учетом того, что мы теперь работаем с 4 потоками,
получается, что проихводительность увеличилась примерно в 3.5 раза, получается, что КПД потока немного, но упал.
Так же можно заметить, что среднее время выполнения запроса и максимальное время исполнения поднялись, даже для
аналогичных нагрузках (20к Stage1 отрабатывает лучше, чем 20к Stage2). Это связано с тем, что PUT - это все же очень легкая
операция для нашего ДАО, поэтому тут накладные расходы на thread executor и синхронизацию многопотока видны как никогда:
в каком-то плане быстрее селектору дождаться пока быстрый пут сделается, чем отдавать запрос в пулл на обработку, а потом
его еще от туда забирать. Однако мы готовы на такую небольшую жертву, ведь при больших нагрузках (70к) наш подход все еще
отрабатывает достойно, когда как Stage1 уже на 25к полностью сдувается.
Про максимальное время выполнение рассуждения аналогичны с GET'ами, однако стоит отметить, что здесь GC, ибо запросы очень
"легкие", меняет максимальное время исполнения очень сильно по сравнению с средним временем исполнения. Так что такая
оптимизация в случае PUT запросов будет еще пользительней GET

Так же можно отметить, что прогрев действительно очень важен как PUT, так и для GET запросов.
1000 непрогретых GET запросов имеет ниже среднюю скорость выполнения как 2000 прогретых запросов
Аналогично 20000 непрогретых PUT аналогичны по скорости 30000 прогретым PUT запросам.
Это, естественно, показывает нам, что Джит реально не зря ресурсы наши жрет, но еще и действительно оптимизирует наш код
на лету, понимая какая нагрузка этот наш код будет ожидать в дальнейшем


PROFILER-GET
Аллокации - на аллкациях GET видно СЛАДЧАЙШЕЕ улучшение. Раньше аллокации шли в шахматном порядке - белый квадрат, красный
квадрат. Это связано с тем, что пока один запрос выполнялся, остальные ждали и поэтому мы аллоцировали что нам надо, ждем
выполнения, потом аллоцируем снова... Теперь же весь график равномерно красный, это показывает, что запросы действительно
выполняются многопоточно и запросы на аллокацию происходят постоянно. Из минусов - нам теперь сложно что улучшить будет
в плане аллкации, все и так очень сладко, разве только постараться уменьшить количество аллокаций в целом
Про CPU можно сказать аналогичную вещь - теперь у нас куда меньше простоя на CPU, мы всегда очень яркие, очень красные,
очень сладкие.
По графику локов можно заметить, что возникают они только в самом начале - когда код еще не прогрет и джит шалит, и в
конце, когда нагрузка максимальная. Возможно можно судить, что появление локов - это показатель того, что все у нас 
становится очень плохо и надо что-то с этим делать, например начинать троттлить и пропускать запросы, или не принимать
новые запросы, отпавлять их на другой, менее загруженный инстанс нашей базы данных

PROFILER-PUT
Аллокации - справедливо то же, что было сказано про GET, но можно еще отметить, что аллокации в PUT были еще более редкими
до этого, чем аллокации в GET, сейчас же они постоянные, это очень славно показывает, что запросы больше не ждут друг друга,
а выполняются синхронно, сладота! Так же на графике выделения памяти PUT запросов видны яркие красные точки - это моменты
флашей на диск.
С CPU все примерно так же, как и с GET, тоже стал куда более равномерно нагруженный график
Локи у путов тоже происходят только когда код не прогрет и джит шалит, и при максимальной нагрузке когда мы уже почти не
справляемся. Тоже можно использовать как индикатор того, что пора закрывать ворота и перенаправлять на более свободного
работягу

Спасибо за прочтение!
